{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNQlYFB3rpeoYBXlKFRlq7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GopikaMunnangi/ElectionResultsPredictor/blob/main/Star.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit pandas scikit-learn xgboost plotly joblib tensorflow transformers sentencepiece"
      ],
      "metadata": {
        "id": "p9sSekwez4Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Cell B: Train models, select best, save outputs ----------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1) Read CSVs\n",
        "poll_path = \"poll_data.csv\"\n",
        "social_path = \"social_media_data.csv\"\n",
        "assert os.path.exists(poll_path), f\"Missing {poll_path}\"\n",
        "assert os.path.exists(social_path), f\"Missing {social_path}\"\n",
        "\n",
        "poll_df = pd.read_csv(poll_path)\n",
        "social_df = pd.read_csv(social_path)\n",
        "\n",
        "# Basic cleaning\n",
        "poll_df = poll_df.dropna(subset=[\"state\",\"candidate\",\"votes\"]).copy()\n",
        "poll_df['votes'] = pd.to_numeric(poll_df['votes'], errors='coerce').fillna(0).astype(int)\n",
        "social_df = social_df.dropna(subset=[\"candidate\",\"comment\"]).copy()\n",
        "social_df['comment'] = social_df['comment'].astype(str)\n",
        "\n",
        "# Sentiment\n",
        "from transformers import pipeline\n",
        "sent_pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "def comment_sentiment_score(text):\n",
        "    out = sent_pipe(text[:512])\n",
        "    lbl = out[0]['label']\n",
        "    sc = float(out[0]['score'])\n",
        "    return sc if lbl == \"POSITIVE\" else -sc\n",
        "\n",
        "tqdm.pandas()\n",
        "social_df['sentiment_score'] = social_df['comment'].progress_apply(comment_sentiment_score)\n",
        "\n",
        "sent_aggr = social_df.groupby('candidate').agg(\n",
        "    sentiment_mean=('sentiment_score','mean'),\n",
        "    comment_count=('comment','count'),\n",
        "    positive_count=('sentiment_score', lambda x: (x>0).sum()),\n",
        "    negative_count=('sentiment_score', lambda x: (x<0).sum()),\n",
        "    neutral_count=('sentiment_score', lambda x: (x==0).sum())\n",
        ").reset_index()\n",
        "\n",
        "df = pd.merge(poll_df, sent_aggr, on='candidate', how='left')\n",
        "df.fillna({'sentiment_mean':0.0,'comment_count':0,'positive_count':0,'negative_count':0,'neutral_count':0}, inplace=True)\n",
        "df[['comment_count','positive_count','negative_count','neutral_count']] = df[['comment_count','positive_count','negative_count','neutral_count']].astype(int)\n",
        "\n",
        "# Label: winner per state\n",
        "df['max_votes_in_state'] = df.groupby('state')['votes'].transform('max')\n",
        "df['winner'] = (df['votes'] == df['max_votes_in_state']).astype(int)\n",
        "\n",
        "# Save processed table\n",
        "processed_path = \"processed_state_candidate_data.csv\"\n",
        "df.to_csv(processed_path, index=False)\n",
        "print(f\"Saved processed data to {processed_path} (rows: {len(df)})\")\n",
        "\n",
        "# 3) Train models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "X = df[['votes','sentiment_mean','comment_count']].copy()\n",
        "y = df['winner']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "joblib.dump(scaler, \"scaler.joblib\")\n",
        "\n",
        "models_results = {}\n",
        "\n",
        "# XGBoost\n",
        "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=150, random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "models_results['XGBoost'] = accuracy_score(y_test, xgb_clf.predict(X_test))\n",
        "joblib.dump(xgb_clf, \"model_xgb.joblib\")\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "models_results['RandomForest'] = accuracy_score(y_test, rf.predict(X_test))\n",
        "joblib.dump(rf, \"model_rf.joblib\")\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "models_results['GradientBoosting'] = accuracy_score(y_test, gb.predict(X_test))\n",
        "joblib.dump(gb, \"model_gb.joblib\")\n",
        "\n",
        "# SVM\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "svm.fit(X_train_scaled, y_train)\n",
        "models_results['SVM'] = accuracy_score(y_test, svm.predict(X_test_scaled))\n",
        "joblib.dump(svm, \"model_svm.joblib\")\n",
        "\n",
        "# FNN\n",
        "tf.random.set_seed(42)\n",
        "fnn = models.Sequential([\n",
        "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "fnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "fnn.fit(X_train_scaled, y_train, validation_split=0.1, epochs=30, batch_size=16, verbose=0)\n",
        "fnn_acc = float(fnn.evaluate(X_test_scaled, y_test, verbose=0)[1])\n",
        "models_results['FNN'] = fnn_acc\n",
        "fnn.save_weights(\"model_fnn.weights.h5\")\n",
        "\n",
        "\n",
        "# -------- FNN vs SVM safeguard --------\n",
        "if abs(models_results['FNN'] - models_results['SVM']) < 1e-4:\n",
        "    # Prefer SVM if both give same accuracy\n",
        "    models_results['FNN'] -= 0.0001\n",
        "\n",
        "import json\n",
        "with open(\"model_accuracies.json\", \"w\") as f:\n",
        "    json.dump(models_results, f, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"model_accuracies.json\", \"r\") as f:\n",
        "    model_accuracies = json.load(f)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = list(model_accuracies.keys())\n",
        "accuracies = list(model_accuracies.values())\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(models, accuracies)\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "plt.xticks(rotation=30, ha=\"right\")\n",
        "\n",
        "for i, acc in enumerate(accuracies):\n",
        "    plt.text(i, acc + 0.01, f\"{acc:.3f}\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "best_model = max(model_accuracies, key=model_accuracies.get)\n",
        "print(f\"Best Model: {best_model} ({model_accuracies[best_model]:.4f})\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ggh1FQK5r14c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "import joblib\n",
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"ElectionPredict\", page_icon=\"ðŸ—³ï¸\", layout=\"wide\")\n",
        "\n",
        "# Styling\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        ".topbar { background-color: #172b4d; padding: 14px; color: white; border-radius:6px;}\n",
        ".hero { text-align:center; padding:50px 0; }\n",
        ".uploader-box { display:flex; justify-content:center; gap:20px; margin-bottom:10px; }\n",
        ".winner { font-family: 'Helvetica', Arial, sans-serif; text-align:center; padding:26px; margin:18px 0; border-radius:12px; background: linear-gradient(90deg, #f6f9ff, #eef6ff); }\n",
        ".winner h1 { font-size:46px; margin:6px; color:#0b3b6f; text-shadow: 0 2px 10px rgba(11,59,111,0.15); }\n",
        ".winner .subtitle { font-size:16px; color:#2f506a; }\n",
        ".glow { animation: glow 1.8s infinite; }\n",
        "@keyframes glow { 0% {text-shadow: 0 0 6px rgba(255,215,0,0.0);} 50% {text-shadow: 0 0 18px rgba(255,215,0,0.5);} 100% {text-shadow: 0 0 6px rgba(255,215,0,0.0);} }\n",
        ".card { padding:12px; border-radius:10px; background:white; box-shadow: 0 4px 14px rgba(12,40,80,0.04); }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "st.markdown('<div class=\"topbar\"><h3 style=\"margin:0\">ElectionPredict</h3></div>', unsafe_allow_html=True)\n",
        "st.markdown('<div class=\"hero\"><h1>Predict Elections With AI</h1><p style=\"color:#415a72\">Advanced analysis using polls + social sentiment.</p></div>', unsafe_allow_html=True)\n",
        "\n",
        "# Upload area\n",
        "st.markdown('<div class=\"uploader-box\">', unsafe_allow_html=True)\n",
        "col1, col2 = st.columns([1,1])\n",
        "with col1:\n",
        "    uploaded_poll = st.file_uploader(\"Upload poll_data.csv\", type=[\"csv\"], help=\"CSV with columns: state,candidate,votes\")\n",
        "with col2:\n",
        "    uploaded_social = st.file_uploader(\"Upload social_media_data.csv\", type=[\"csv\"], help=\"CSV with columns: candidate,comment\")\n",
        "st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "start = st.button(\"Start Generating Predictions\", key=\"start_gen\")\n",
        "\n",
        "@st.cache_resource\n",
        "def compute_sentiment_df(social_df):\n",
        "    sent_pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "    scores = []\n",
        "    for txt in social_df['comment'].astype(str).tolist():\n",
        "        out = sent_pipe(txt[:512])\n",
        "        lbl = out[0]['label']\n",
        "        sc = float(out[0]['score'])\n",
        "        scores.append(sc if lbl == \"POSITIVE\" else -sc)\n",
        "    social_df = social_df.copy()\n",
        "    social_df['sentiment_score'] = scores\n",
        "    agg = social_df.groupby('candidate').agg(\n",
        "        sentiment_mean=('sentiment_score','mean'),\n",
        "        comment_count=('comment','count'),\n",
        "        positive_count=('sentiment_score', lambda x: (x>0).sum()),\n",
        "        negative_count=('sentiment_score', lambda x: (x<0).sum()),\n",
        "        neutral_count=('sentiment_score', lambda x: (x==0).sum())\n",
        "    ).reset_index()\n",
        "    return agg\n",
        "\n",
        "processed_path = \"processed_state_candidate_data.csv\"\n",
        "use_processed = (os.path.exists(processed_path) and (uploaded_poll is None and uploaded_social is None))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_fnn():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Input(shape=(3,)),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(32, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.2),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "      ])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "  model.load_weights(\"model_fnn.weights.h5\")\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def load_models_and_scaler():\n",
        "    models = {\n",
        "        \"XGBoost\": joblib.load(\"model_xgb.joblib\"),\n",
        "        \"RandomForest\": joblib.load(\"model_rf.joblib\"),\n",
        "        \"GradientBoosting\": joblib.load(\"model_gb.joblib\"),\n",
        "        \"SVM\": joblib.load(\"model_svm.joblib\"),\n",
        "\n",
        "        \"FNN\": build_fnn()\n",
        "\n",
        "\n",
        "    }\n",
        "    scaler = joblib.load(\"scaler.joblib\")\n",
        "\n",
        "    with open(\"model_accuracies.json\") as f:\n",
        "        accuracies = json.load(f)\n",
        "\n",
        "    return models, scaler, accuracies\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if start:\n",
        "    # Load data\n",
        "    if use_processed:\n",
        "        st.info(\"Loading previously processed dataset from Colab.\")\n",
        "        df = pd.read_csv(processed_path)\n",
        "    else:\n",
        "        if uploaded_poll is None or uploaded_social is None:\n",
        "            st.error(\"Please upload both CSV files or ensure processed_state_candidate_data.csv exists in Colab.\")\n",
        "            st.stop()\n",
        "        poll_df = pd.read_csv(uploaded_poll)\n",
        "        social_df = pd.read_csv(uploaded_social)\n",
        "        poll_df = poll_df.dropna(subset=[\"state\",\"candidate\",\"votes\"]).copy()\n",
        "        poll_df['votes'] = pd.to_numeric(poll_df['votes'], errors='coerce').fillna(0).astype(int)\n",
        "        social_df = social_df.dropna(subset=[\"candidate\",\"comment\"]).copy()\n",
        "        social_df['comment'] = social_df['comment'].astype(str)\n",
        "        with st.spinner(\"Computing transformer sentiment for social comments...\"):\n",
        "            sent_aggr = compute_sentiment_df(social_df)\n",
        "        df = pd.merge(poll_df, sent_aggr, on='candidate', how='left')\n",
        "        df['sentiment_mean'] = df['sentiment_mean'].fillna(0.0)\n",
        "        df['comment_count'] = df['comment_count'].fillna(0).astype(int)\n",
        "        df['positive_count'] = df['positive_count'].fillna(0).astype(int)\n",
        "        df['negative_count'] = df['negative_count'].fillna(0).astype(int)\n",
        "        df['neutral_count'] = df['neutral_count'].fillna(0).astype(int)\n",
        "\n",
        "    df['max_votes_in_state'] = df.groupby('state')['votes'].transform('max')\n",
        "    df['actual_winner'] = (df['votes'] == df['max_votes_in_state']).astype(int)\n",
        "\n",
        "    # State-level results\n",
        "    st.markdown(\"## State-level Results\")\n",
        "    pivot = df.pivot_table(index='state', columns='candidate', values='votes', aggfunc='sum', fill_value=0)\n",
        "    winners = pivot.idxmax(axis=1).rename(\"Winner\")\n",
        "    state_table = pivot.copy()\n",
        "    state_table['Winner'] = winners\n",
        "    st.dataframe(state_table.reset_index().sort_values('state'))\n",
        "\n",
        "    # Candidate summary\n",
        "    st.markdown(\"## Candidate Summary (Aggregated across all states)\")\n",
        "    agg = df.groupby('candidate').agg(\n",
        "        total_votes=('votes','sum'),\n",
        "        avg_sentiment=('sentiment_mean','mean'),\n",
        "        total_comments=('comment_count','sum'),\n",
        "        positive_comments=('positive_count','sum'),\n",
        "        negative_comments=('negative_count','sum')\n",
        "    ).reset_index().sort_values('total_votes', ascending=False)\n",
        "    st.dataframe(agg)\n",
        "\n",
        "    # Visualizations\n",
        "    st.markdown(\"## Visualizations\")\n",
        "    colA, colB = st.columns([1,1])\n",
        "    with colA:\n",
        "        fig = go.Figure(data=[go.Pie(labels=agg['candidate'], values=agg['total_votes'], hole=.48)])\n",
        "        fig.update_layout(title_text=\"Overall Vote Share (Polls)\")\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "    with colB:\n",
        "        fig2 = px.bar(agg, x='candidate', y='total_votes', title=\"Total Votes by Candidate\")\n",
        "        st.plotly_chart(fig2, use_container_width=True)\n",
        "\n",
        "    st.markdown(\"### Sentiment breakdown\")\n",
        "    sent_df = agg[['candidate','positive_comments','negative_comments']].melt(id_vars='candidate', var_name='type', value_name='count')\n",
        "    fig3 = px.bar(sent_df, x='candidate', y='count', color='type', barmode='stack', title=\"Sentiment counts per candidate\")\n",
        "    st.plotly_chart(fig3, use_container_width=True)\n",
        "\n",
        "\n",
        "    # -------- ML Prediction inside Streamlit --------\n",
        "    models, scaler, accuracies = load_models_and_scaler()\n",
        "    # Select best model\n",
        "    best_model_name = max(accuracies, key=accuracies.get)\n",
        "    best_model = models[best_model_name]\n",
        "    X = df[['votes','sentiment_mean','comment_count']].copy()\n",
        "    if best_model_name == \"SVM\":\n",
        "      X_scaled = scaler.transform(X)\n",
        "      probs = best_model.predict_proba(X_scaled)[:, 1]\n",
        "    elif best_model_name == \"FNN\":\n",
        "      X_scaled = scaler.transform(X)\n",
        "      probs = best_model.predict(X_scaled).flatten()\n",
        "    else:\n",
        "      probs = best_model.predict_proba(X)[:, 1]\n",
        "\n",
        "    df['predicted_prob'] = probs\n",
        "    df['weighted_votes'] = df['votes'] * df['predicted_prob']\n",
        "    pred_votes = (\n",
        "        df.groupby('candidate')['weighted_votes']\n",
        "          .sum()\n",
        "          .reset_index()\n",
        "          .sort_values('weighted_votes', ascending=False)\n",
        "    )\n",
        "    if not pred_votes.empty:\n",
        "      ml_winner = pred_votes.iloc[0]['candidate']\n",
        "      ml_votes = int(pred_votes.iloc[0]['weighted_votes'])\n",
        "    else:\n",
        "      ml_winner = \"No predicted winner\"\n",
        "      ml_votes = 0\n",
        "\n",
        "\n",
        "\n",
        "    # Final winner (ML-based)\n",
        "    st.markdown(\"## Final Predicted Winner\")\n",
        "    st.markdown(f\"\"\"\n",
        "    <div class=\"winner\">\n",
        "      <div style=\"text-align:center\">\n",
        "         <h1 class=\"glow\">ðŸ¥‡ {ml_winner.upper()}</h1>\n",
        "         <div style=\"font-size:18px;padding-top:8px;color:#0b3b6f\">Predicted weighted votes: {ml_votes}</div>\n",
        "      </div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)"
      ],
      "metadata": {
        "id": "O_AfT5Scl23D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell D â€” launch streamlit and print proxy link\n",
        "import time, subprocess, os\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "# kill existing streamlit (if any)\n",
        "!kill -9 $(lsof -t -i:8501) 2>/dev/null || true\n",
        "\n",
        "# start streamlit in background\n",
        "get_ipython().system_raw('nohup streamlit run app.py --server.enableCORS false --server.enableXsrfProtection false --server.headless true > /dev/null 2>&1 &')\n",
        "time.sleep(4)\n",
        "print(\"Open this link in your browser:\")\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(8501)\"))"
      ],
      "metadata": {
        "id": "eGHrGDlrEYwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "states = [\"Andhra Pradesh\", \"Telangana\", \"Tamil Nadu\", \"Karnataka\", \"Kerala\", \"Maharashtra\", \"Gujarat\", \"West Bengal\", \"Rajasthan\", \"Bihar\"]\n",
        "candidates = [\"Srikanth Naidu\", \"Mahesh Naidu\", \"Sriya Patel\"]\n",
        "\n",
        "data = {\n",
        "    \"state\": [],\n",
        "    \"candidate\": [],\n",
        "    \"votes\": []\n",
        "}\n",
        "\n",
        "# Generate 150 entries\n",
        "for _ in range(150):\n",
        "    state = random.choice(states)\n",
        "    candidate = random.choice(candidates)\n",
        "    votes = random.randint(1500, 6000)  # random votes between 1500-6000\n",
        "    data[\"state\"].append(state)\n",
        "    data[\"candidate\"].append(candidate)\n",
        "    data[\"votes\"].append(votes)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save CSV\n",
        "df.to_csv(\"poll_data_large_sample.csv\", index=False)\n",
        "\n",
        "# For Colab: trigger download\n",
        "from google.colab import files\n",
        "files.download(\"poll_data_large_sample.csv\")\n"
      ],
      "metadata": {
        "id": "ILerGhjwC1kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "candidates = [\"Srikanth Naidu\", \"Mahesh Naidu\", \"Sriya Patel\"]\n",
        "comments_sample = [\n",
        "    \"is a strong leader with an inspiring vision.\",\n",
        "    \"understands economic issues well.\",\n",
        "    \"seems honest and dependable.\",\n",
        "    \"should win this election, he brings stability.\",\n",
        "    \"might actually surprise everyone.\",\n",
        "    \"has gained attention in the state.\",\n",
        "    \"is energetic and fresh.\",\n",
        "    \"is not well-known but improves every debate.\",\n",
        "    \"will grow more in future elections.\",\n",
        "    \"brings innovation and new thinking.\",\n",
        "    \"has a clear plan for development.\",\n",
        "    \"experience in politics is unmatched.\",\n",
        "    \"is the best choice for progress.\",\n",
        "    \"policies on education are excellent.\",\n",
        "    \"is dynamic and relatable.\",\n",
        "    \"has great ideas for women's rights.\"\n",
        "]\n",
        "\n",
        "data = {\n",
        "    \"candidate\": [],\n",
        "    \"comment\": []\n",
        "}\n",
        "\n",
        "# Generate 150 comments\n",
        "for _ in range(150):\n",
        "    candidate = random.choice(candidates)\n",
        "    comment = candidate + \" \" + random.choice(comments_sample)\n",
        "    data[\"candidate\"].append(candidate)\n",
        "    data[\"comment\"].append(comment)\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save CSV\n",
        "df.to_csv(\"social_media_data_large_sample.csv\", index=False)\n",
        "\n",
        "# For Colab: trigger download\n",
        "from google.colab import files\n",
        "files.download(\"social_media_data_large_sample.csv\")\n"
      ],
      "metadata": {
        "id": "50uLnC2KDGkS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}